{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIA48NLvNLBV"
      },
      "source": [
        "# **1. Determine the data set and process it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FURP31As2gpi"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg-jIh9j2igq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aGk8qH7Eykn"
      },
      "source": [
        "**Load datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIU5CSw6El3p",
        "outputId": "4a195b49-0a8a-477e-c249-d70a0221c51b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Any', 'Data', 'Codi_Postal', 'Sector_Economic', 'Tram_Horari',\n",
            "       'Valor'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('2019-2022_consum_electricitat_bcn.csv',sep=\",\")\n",
        "df1 = pd.read_csv('2023-2024_consum_electricitat_bcn.csv',sep=\",\")\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNq90de3FcYq"
      },
      "source": [
        "**Join the two dataframes by concatenation, as they contain the same columns.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtLyy7N_FZ6j"
      },
      "outputs": [],
      "source": [
        "df1 = df1.iloc[1:] # Remove the first row of the consumption data for 2023-2024 as it contains the column names\n",
        "df = pd.concat([df, df1], axis=0, ignore_index=True) # Concatenate the two dataframes\n",
        "df_consum = df.drop(columns=['Any']) # Remove the 'Any' column as we already have that in the date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4TjSd94GGG5",
        "outputId": "2f6fd35b-122f-4535-f799-1195acf09cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              Data               Tram_Horari  Codi_Postal   Valor\n",
            "0       2019-01-01  De 00:00:00 a 05:59:59 h         8001   68681\n",
            "1       2019-01-01  De 00:00:00 a 05:59:59 h         8002  101754\n",
            "2       2019-01-01  De 00:00:00 a 05:59:59 h         8003  106116\n",
            "3       2019-01-01  De 00:00:00 a 05:59:59 h         8004  110622\n",
            "4       2019-01-01  De 00:00:00 a 05:59:59 h         8005  113188\n",
            "...            ...                       ...          ...     ...\n",
            "436165  2024-10-31                 No consta         8038   13589\n",
            "436166  2024-10-31                 No consta         8039   22674\n",
            "436167  2024-10-31                 No consta         8040   34629\n",
            "436168  2024-10-31                 No consta         8041    2322\n",
            "436169  2024-10-31                 No consta         8042     724\n",
            "\n",
            "[436170 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# Group by 'tram_horari', 'data', and 'codi_postal', summing 'Valor'\n",
        "df_agrupat = df_consum.groupby(['Data', 'Tram_Horari', 'Codi_Postal'], as_index=False).agg({'Valor': 'sum'})\n",
        "\n",
        "print(df_agrupat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avVFixLBHLPh"
      },
      "source": [
        "We have a problem, as the time slot is missing for some values. We will solve the problem by dividing the energy demand value between the different time slots, proportionally to the value of the time slot. This way, we will no longer have samples without a time slot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gCYyXe98HKIq",
        "outputId": "3f2b54ad-44ab-429d-d181-16f75dbf9e6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              Data               Tram_Horari  Codi_Postal          Valor\n",
            "0       2019-01-01  De 00:00:00 a 05:59:59 h         8001   71768.816539\n",
            "1       2019-01-01  De 00:00:00 a 05:59:59 h         8002  105525.771005\n",
            "2       2019-01-01  De 00:00:00 a 05:59:59 h         8003  109681.449902\n",
            "3       2019-01-01  De 00:00:00 a 05:59:59 h         8004  114701.832375\n",
            "4       2019-01-01  De 00:00:00 a 05:59:59 h         8005  117194.598557\n",
            "...            ...                       ...          ...            ...\n",
            "436123  2024-10-31  De 18:00:00 a 23:59:59 h         8038  141482.554850\n",
            "436124  2024-10-31  De 18:00:00 a 23:59:59 h         8039  153705.681040\n",
            "436125  2024-10-31  De 18:00:00 a 23:59:59 h         8040  244330.340099\n",
            "436126  2024-10-31  De 18:00:00 a 23:59:59 h         8041   68883.106750\n",
            "436127  2024-10-31  De 18:00:00 a 23:59:59 h         8042   39368.640974\n",
            "\n",
            "[348936 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# Filter the rows where 'Tram_Horari' is 'No consta'\n",
        "df_no_tram = df_agrupat[df_agrupat['Tram_Horari'] == 'No consta']\n",
        "\n",
        "# Filter the rows with assigned time slots\n",
        "df_amb_tram = df_agrupat[df_agrupat['Tram_Horari'] != 'No consta']\n",
        "\n",
        "# Distribute the values of 'df_no_tram' proportionally across the time slots\n",
        "def repartir_valor(fila_no_tram):\n",
        "    data = fila_no_tram['Data']\n",
        "    codi_postal = fila_no_tram['Codi_Postal']\n",
        "    valor_sense_tram = fila_no_tram['Valor']\n",
        "\n",
        "    # Filter the time slots for that date and postal code\n",
        "    franjes_disponibles = df_amb_tram[(df_amb_tram['Data'] == data) & (df_amb_tram['Codi_Postal'] == codi_postal)]\n",
        "\n",
        "    # Calculate the sum of the values for the available time slots\n",
        "    suma_franjes = franjes_disponibles['Valor'].sum()\n",
        "\n",
        "    if suma_franjes > 0:\n",
        "        # Calculate the proportion and distribute\n",
        "        proporcio = franjes_disponibles['Valor'] / suma_franjes\n",
        "        repartiment = proporcio * valor_sense_tram\n",
        "\n",
        "        # Add the distributed value to the existing time slots\n",
        "        df_amb_tram.loc[df_amb_tram.index.isin(franjes_disponibles.index), 'Valor'] += repartiment\n",
        "\n",
        "# Apply the distribution function to each row of df_no_tram\n",
        "df_no_tram.apply(repartir_valor, axis=1)\n",
        "\n",
        "# Combine the rows with the distributed values and the original rows\n",
        "df_final = pd.concat([df_amb_tram, df_no_tram])\n",
        "\n",
        "# Remove the rows where 'Tram_Horari' is 'No consta', as the value has already been distributed\n",
        "df_final = df_final[df_final['Tram_Horari'] != 'No consta']\n",
        "\n",
        "print(df_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lrsi8xlHi01"
      },
      "source": [
        "Review if we have solved the **problem**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ_s2_KTHm11"
      },
      "outputs": [],
      "source": [
        "print('Initial dataframe:') # Tram Horari with \"no consta\" values\n",
        "print(df_agrupat[(df_agrupat['Data']==\"2021-04-18\") & (df_agrupat['Codi_Postal']==8001)])\n",
        "\n",
        "print('-----------------------------------------------------------------------')\n",
        "\n",
        "print('Final dataframe:') # Tram Horari without \"no consta\" values\n",
        "print(df_final[(df_final['Data']==\"2021-04-18\") & (df_final['Codi_Postal']==8001)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wn-H5tYIHsB"
      },
      "source": [
        "Load CSVs with data that provide more information to the initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyHmZl6nIGq8"
      },
      "outputs": [],
      "source": [
        "# Import Calendar (weekday and whether it's considered a holiday or not)\n",
        "df = pd.read_csv('2019-2024_Calendari.csv', sep=\";\")\n",
        "df[\"Data\"] = pd.to_datetime(df[\"Data\"], format=\"%d/%m/%Y\").dt.strftime(\"%Y-%m-%d\")\n",
        "df = df.rename(columns={'Data': 'Date'})\n",
        "\n",
        "# Import IPI (Industrial Production Index can have a big impact on energy consumption)\n",
        "df1 = pd.read_csv('tasa_interanual_del_indice_de_produccion_industrial_en_catalu√±a.csv', sep=\";\")\n",
        "df1 = df1.dropna(subset=['Periodo'])\n",
        "\n",
        "# Convert 'Periodo' to a numeric month for merging purposes.\n",
        "month_mapping = {\n",
        "    'Enero': 1, 'Febrero': 2, 'Marzo': 3, 'Abril': 4, 'Mayo': 5,\n",
        "    'Junio': 6, 'Julio': 7, 'Agosto': 8, 'Septiembre': 9, 'Octubre': 10,\n",
        "    'Noviembre': 11, 'Diciembre': 12\n",
        "}\n",
        "df1['Periodo'] = df1['Periodo'].map(month_mapping)\n",
        "\n",
        "# Create a new column from the year and period, to match the format of the other df\n",
        "df1['Date'] = pd.to_datetime(df1['A√±o'].astype(str) + '-' + df1['Periodo'].astype(str) + '-01') # Assign day 1, it's not relevant\n",
        "df1 = df1.drop(['A√±o', 'Periodo'], axis=1) # Remove columns that are no longer informative\n",
        "df1 = df1[['Date', 'Tasa interanual del IPI']] # Not necessary, as the other columns were already dropped, it's to ensure the result\n",
        "\n",
        "# Display the result from the CSVs\n",
        "print(df.head()) # Calendar\n",
        "print(\"------------------------------------\")\n",
        "print(df1.head()) # IPI\n",
        "\n",
        "# Merge\n",
        "# Ensure the format of the data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df1['Date'] = pd.to_datetime(df1['Date'])\n",
        "\n",
        "df['Year_Month'] = df['Date'].dt.to_period('M')  # To get just the year and month\n",
        "df1['Year_Month'] = df1['Date'].dt.to_period('M')  # To get just the year and month\n",
        "\n",
        "# Merge the two dataframes based on the 'Year_Month' column\n",
        "df2 = pd.merge(df, df1, on='Year_Month', how='left')\n",
        "df2 = df2.drop(['Date_y', 'Year_Month'], axis=1)\n",
        "df2.rename(columns={'Date_x': 'Date'}, inplace=True)\n",
        "\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(df2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viMGEKwyIUAy"
      },
      "source": [
        "We will connect to a weather API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Fe9jeIbXJSzj"
      },
      "outputs": [],
      "source": [
        "pip install openmeteo-requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "03qX63PUJYZd"
      },
      "outputs": [],
      "source": [
        "pip install requests-cache retry-requests numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPKjG_XVITxU"
      },
      "outputs": [],
      "source": [
        "# API\n",
        "import openmeteo_requests\n",
        "import requests_cache\n",
        "from retry_requests import retry\n",
        "\n",
        "# Setup the Open-Meteo API client with cache and retry on error\n",
        "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
        "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
        "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
        "\n",
        "# Make sure all required weather variables are listed here\n",
        "# The order of variables in hourly or daily is important to assign them correctly below\n",
        "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "params = {\n",
        "\t\"latitude\": 41.39978186013002,\n",
        "\t\"longitude\": 2.151839058933601,\n",
        "\t\"start_date\": \"2019-01-01\",\n",
        "\t\"end_date\": \"2024-12-24\",\n",
        "\t\"hourly\": [\"temperature_2m\", \"apparent_temperature\", \"rain\", \"wind_speed_10m\", \"is_day\", \"sunshine_duration\", \"direct_radiation\",'dew_point_2m'],\n",
        "\t\"temporal_resolution\": \"hourly_6\"\n",
        "}\n",
        "responses = openmeteo.weather_api(url, params=params)\n",
        "\n",
        "# Process first location. Add a for-loop for multiple locations or weather models\n",
        "response = responses[0]\n",
        "print(f\"Coordinates {response.Latitude()}¬∞N {response.Longitude()}¬∞E\")\n",
        "print(f\"Elevation {response.Elevation()} m asl\")\n",
        "\n",
        "# Process hourly data. The order of variables needs to be the same as requested.\n",
        "hourly = response.Hourly()\n",
        "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
        "hourly_apparent_temperature = hourly.Variables(1).ValuesAsNumpy()\n",
        "hourly_rain = hourly.Variables(2).ValuesAsNumpy()\n",
        "hourly_wind_speed_10m = hourly.Variables(3).ValuesAsNumpy()\n",
        "hourly_is_day = hourly.Variables(4).ValuesAsNumpy()\n",
        "hourly_sunshine_duration = hourly.Variables(5).ValuesAsNumpy()\n",
        "hourly_direct_radiation = hourly.Variables(6).ValuesAsNumpy()\n",
        "hourly_dew_point_2m = hourly.Variables(7).ValuesAsNumpy()\n",
        "\n",
        "hourly_data = {\"date\": pd.date_range(\n",
        "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
        "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
        "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
        "\tinclusive = \"left\"\n",
        ")}\n",
        "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
        "hourly_data[\"apparent_temperature\"] = hourly_apparent_temperature\n",
        "hourly_data[\"rain\"] = hourly_rain\n",
        "hourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
        "hourly_data[\"is_day\"] = hourly_is_day\n",
        "hourly_data[\"sunshine_duration\"] = hourly_sunshine_duration\n",
        "hourly_data[\"direct_radiation\"] = hourly_direct_radiation\n",
        "hourly_data[\"dew_point_2m\"] = hourly_dew_point_2m\n",
        "\n",
        "\n",
        "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
        "\n",
        "# Convert the 'date' column to datetime type for later processing\n",
        "hourly_dataframe['date'] = pd.to_datetime(hourly_dataframe['date'])\n",
        "\n",
        "# Define a function to determine the time slot\n",
        "def determinar_franja_horaria(date):\n",
        "    hora = date.hour\n",
        "    if 0 <= hora < 6:\n",
        "        return \"From 00:00:00 to 05:59:59\"\n",
        "    elif 6 <= hora < 12:\n",
        "        return \"From 06:00:00 to 11:59:59\"\n",
        "    elif 12 <= hora < 18:\n",
        "        return \"From 12:00:00 to 17:59:59\"\n",
        "    elif 18 <= hora < 24:\n",
        "        return \"From 18:00:00 to 23:59:59\"\n",
        "\n",
        "# Apply the function to create the time slot column\n",
        "hourly_dataframe[\"t_Horari\"] = hourly_dataframe[\"date\"].apply(determinar_franja_horaria)\n",
        "\n",
        "# Format the date to the 'YYYY-MM-DD' format, so we have a common column for the join\n",
        "hourly_dataframe[\"date\"] = hourly_dataframe[\"date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "print(hourly_dataframe.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVvfrMnOIdpS"
      },
      "source": [
        "Join with electricity consumption data, created previously, to later analyze which features have a greater impact on consumption,discriminating the insignificant ones and processing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LWSpFEEIeJd"
      },
      "outputs": [],
      "source": [
        "# Merge the consumption dataframe with the weather dataframe, it must match both the time slot and the date\n",
        "df_conjunta1 = pd.merge(\n",
        "    df_final,\n",
        "    hourly_dataframe,\n",
        "    left_on=[\"Data\", \"Tram_Horari\"],\n",
        "    right_on=[\"date\", \"t_Horari\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Merge the consumption and weather dataframe with the calendar and IPI dataframe\n",
        "df_conjunta1['date'] = pd.to_datetime(df_conjunta1['date'], errors='coerce') # Ensure the correct data format\n",
        "df2['Date'] = pd.to_datetime(df2['Date'], errors='coerce') # Ensure the correct data format\n",
        "\n",
        "df_conjunta2 = pd.merge(\n",
        "    df_conjunta1,\n",
        "    df2,\n",
        "    left_on=\"date\",\n",
        "    right_on=\"Date\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Order the remaining columns\n",
        "df_conjunta3 = df_conjunta2[['date','Tram_Horari','Codi_Postal', 'Valor', 'temperature_2m','apparent_temperature','rain','wind_speed_10m','is_day','sunshine_duration','direct_radiation','Dia_Setmana','Festiu','Tasa interanual del IPI','dew_point_2m']]\n",
        "\n",
        "# Process the columns as they are needed for further use\n",
        "df_conjunta3['rain'] = df_conjunta3['rain'].astype(bool)\n",
        "df_conjunta3['is_day'] = df_conjunta3['is_day'].astype(bool)\n",
        "df_conjunta3['Festiu'] = df_conjunta3['Festiu'].astype(bool)\n",
        "df_conjunta3['Tasa interanual del IPI'] = df_conjunta3['Tasa interanual del IPI'].str.replace(',', '.').astype(float)\n",
        "df_conjunta3['date'] = pd.to_datetime(df_conjunta3['date'])\n",
        "\n",
        "# Function to convert the \"Tram_Horari\" string to a categorical value\n",
        "def determinar_franja_horaria(tram_horari):\n",
        "    if \"De 00:00:00 a 05:59:59 h\" in tram_horari:\n",
        "        return 1\n",
        "    elif \"De 06:00:00 a 11:59:59 h\" in tram_horari:\n",
        "        return 2\n",
        "    elif \"De 12:00:00 a 17:59:59 h\" in tram_horari:\n",
        "        return 3\n",
        "    elif \"De 18:00:00 a 23:59:59 h\" in tram_horari:\n",
        "        return 4\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "df_conjunta4 = df_conjunta3 # To avoid modifying the original variable\n",
        "\n",
        "# Apply the function to create the time slot column\n",
        "df_conjunta4['Tram_Horari'] = df_conjunta3['Tram_Horari'].apply(determinar_franja_horaria)\n",
        "\n",
        "print(df_conjunta3.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CM7ksaJIjds"
      },
      "outputs": [],
      "source": [
        "# Function to convert the \"Dia_Setmana\" string to categorical\n",
        "df_conjunta5 = df_conjunta3  # To avoid modifying the original variable\n",
        "df_conjunta5['Dia_Setmana'] = df_conjunta5['Dia_Setmana'].astype(str)  # Check the data format\n",
        "\n",
        "def determinar_dia_setmana(Dia_Setmana):\n",
        "    if \"Dilluns\" in Dia_Setmana:\n",
        "        return 1\n",
        "    elif \"Dimarts\" in Dia_Setmana:\n",
        "        return 2\n",
        "    elif \"Dimecres\" in Dia_Setmana:\n",
        "        return 3\n",
        "    elif \"Dijous\" in Dia_Setmana:\n",
        "        return 4\n",
        "    elif \"Divendres\" in Dia_Setmana:\n",
        "        return 5\n",
        "    elif \"Dissabte\" in Dia_Setmana:\n",
        "        return 6\n",
        "    elif \"Diumenge\" in Dia_Setmana:\n",
        "        return 7\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Apply the function to create the weekday column\n",
        "df_conjunta5['Dia_Setmana'] = df_conjunta5['Dia_Setmana'].apply(determinar_dia_setmana)\n",
        "\n",
        "# Modify the format of the 'date' column to extract more information, as the moment has a large impact.\n",
        "df_conjunta6 = df_conjunta5  # To avoid modifying the original variable\n",
        "df_conjunta6['Any'] = df_conjunta6['date'].dt.year\n",
        "df_conjunta6['Mes'] = df_conjunta6['date'].dt.month\n",
        "df_conjunta6['Dia'] = df_conjunta6['date'].dt.day\n",
        "\n",
        "# Order the columns, only keep the ones that provide information\n",
        "df_conjunta6 = df_conjunta6[['Any', 'Mes', 'Dia', 'Tram_Horari', 'Codi_Postal', 'Valor', 'temperature_2m', 'apparent_temperature', 'rain', 'wind_speed_10m', 'is_day', 'sunshine_duration', 'direct_radiation', 'Dia_Setmana', 'Festiu', 'Tasa interanual del IPI', 'dew_point_2m']]\n",
        "print(df_conjunta6.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl9yvSY8ItAh"
      },
      "source": [
        "Handle the outliers that introduce noise into our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXa-TNrpIqri"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 3))\n",
        "sns.boxplot(x=df_conjunta6['Valor'])\n",
        "plt.title('Boxplot of the Value')\n",
        "plt.show()\n",
        "\n",
        "# Calculation of IQR (Interquartile Range)\n",
        "Q1 = df_conjunta6['Valor'].quantile(0.25)\n",
        "Q3 = df_conjunta6['Valor'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the limits for outliers\n",
        "lower_bound = Q1 - 1.5 * IQR # Not necessary as we don't have values lower than 0\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = df_conjunta6[(df_conjunta6['Valor'] > upper_bound)] # Filter the outliers from the original dataset\n",
        "\n",
        "# Display the outliers\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"25% of the data: \", Q1)\n",
        "print(\"75% of the data: \", Q3)\n",
        "print(\"Upper Bound: \", upper_bound)\n",
        "print(\"Outliers: \", outliers['Valor'].count())\n",
        "print(\"Total Dataset: \", df_conjunta6['Valor'].count())\n",
        "print(\"Using \", 100 - (outliers['Valor'].count() / df_conjunta6['Valor'].count()) * 100, \"% of the original dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QWTz1DzIxGn"
      },
      "source": [
        "Eliminate outliers that are outside the noise contained in the energy demand and do not represent the norm, while keeping some of the atypical values that could be noise associated with the phenomenon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHiQKWaHIwpw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 3))\n",
        "sns.boxplot(x=outliers['Valor'])\n",
        "plt.title('Boxplot of the outliers')\n",
        "plt.show()\n",
        "\n",
        "Q1 = outliers['Valor'].quantile(0.25)\n",
        "Q3 = outliers['Valor'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "print(\"Upper Bound: \", upper_bound)\n",
        "\n",
        "outliers = outliers[(outliers['Valor'] > upper_bound)]\n",
        "\n",
        "df_cleaned = df_conjunta6[(df_conjunta6['Valor'] <= upper_bound)]\n",
        "deleted = df_conjunta6[(df_conjunta6['Valor'] > upper_bound)]\n",
        "\n",
        "print(\"Deleted Dataset: \", deleted['Valor'].count())\n",
        "print(\"Total Dataset: \", df_cleaned['Valor'].count())\n",
        "print(\"Using \", 100 - (deleted['Valor'].count() / df_conjunta6['Valor'].count()) * 100, \"% of the original dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuNOisPfI36n"
      },
      "source": [
        "Create a .csv with the dataframe df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoAwXPgXI3mR"
      },
      "outputs": [],
      "source": [
        "df_cleaned.to_csv(\"dataframe_final.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FwX5xcIKuSf"
      },
      "outputs": [],
      "source": [
        "print(df_cleaned.describe())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}